{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/marlonmalheiros/simple-baseline-for-estimation?scriptVersionId=221129393\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 1 - Config","metadata":{}},{"cell_type":"code","source":"# 1.1: Basic Notebook Configuration\n# - Automatic reload of modules\n# - Inline plotting\n\n# %reload_ext autoreload\n# %autoreload 2\n# %matplotlib inline\n\nimport os\nimport json\nimport cv2\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.models.resnet import resnet50  # We'll adapt the ResNet backbone\nfrom torch.utils.data import random_split, DataLoader\n\nimport torch.backends.cudnn as cudnn\n\nimport time\n\ncudnn.benchmark = True\n\n# If you have a custom model file:\n# from my_models import SimpleBaselinePose  # Example name for your pose model\n# 2.1: Define key parameters\n\n# Data paths\nCOCO_IMAGES_ROOT = r\"/kaggle/input/coco-2017-dataset/coco2017/train2017\"      # e.g., COCO train2017 images\nCOCO_ANNOTATIONS = r\"/kaggle/input/coco-2017-dataset/coco2017/annotations/person_keypoints_train2017.json\" # e.g., COCO annotations\n\n# Val files for quick tests\n# COCO_IMAGES_ROOT = r\"C:\\Users\\Marlon\\Downloads\\val2017\"      # e.g., COCO train2017 images\n# COCO_ANNOTATIONS = r\"c:\\Users\\Marlon\\Downloads\\annotations\\person_keypoints_val2017.json\" # e.g., COCO annotations\n\n# Training details from the paper\nIMG_WIDTH = 192\nIMG_HEIGHT = 256\nASPECT_RATIO = (4, 3)    # height : width\nSCALE_AUG = 0.3          # ±30%\nROTATION_AUG = 40.0      # ±40 degrees\nFLIP_PROB = 0.5          # 50% horizontal flip\n\n# Optimizer parameters\nBASE_LR = 1e-3 # Authors use 1, im tweaking to increase speed\nLR_MILESTONES = [50, 75]    # epochs at which LR drops to 1e-4, then 1e-5, original [90, 120]\nNUM_EPOCHS = 100 # Original 140\nBATCH_SIZE = 256 # Authors use 128\n\n# Number of keypoints in COCO is 17\nNUM_KEYPOINTS = 17\n\n# GPU usage\n# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nNUM_WORKERS = 4  # For DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T03:35:46.840773Z","iopub.execute_input":"2025-02-06T03:35:46.841151Z","iopub.status.idle":"2025-02-06T03:35:46.848014Z","shell.execute_reply.started":"2025-02-06T03:35:46.841119Z","shell.execute_reply":"2025-02-06T03:35:46.847035Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# 2 - Class dataset","metadata":{}},{"cell_type":"code","source":"class COCOPoseDataset(Dataset):\n    def __init__(self, annotations_json, images_root, \n                 transform=None, input_size=(256,192), \n                 scale_aug=0.3, rotation_aug=40, flip_prob=0.5,\n                 min_box_side=4):\n        \n        \"\"\"\n        annotations_json: Path to a COCO-format JSON that includes 'images' and 'annotations'\n        images_root: The directory with the actual .jpg files\n        min_box_side: The minimum bounding box dimension (width or height) after aspect ratio correction.\n                      If it's below this value, we skip that annotation.\n        \"\"\"\n        \n        self.images_root = images_root\n        self.transform = transform\n        self.input_size = input_size  # (height=256, width=192)\n        self.scale_aug = scale_aug\n        self.rotation_aug = rotation_aug\n        self.flip_prob = flip_prob\n        \n        self.num_keypoints = 17  # COCO\n        self.sigma = 2  # for generating heatmaps\n        self.min_box_side = min_box_side\n        \n        # 1. Load and filter the annotations in init:\n        self.annotations = self._load_and_filter_coco(annotations_json)\n        \n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        ann = self.annotations[idx]\n        image_path = os.path.join(self.images_root, ann[\"file_name\"])\n        \n        img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Bbox is already aspect-ratio adjusted and guaranteed to be >= min_box_side\n        # according to our init filtering.\n        bbox = ann[\"bbox\"]  # [x, y, w, h], 4:3, clipped\n        x, y, w, h = bbox\n        \n        # -- Crop --\n        cropped = img[int(y):int(y+h), int(x):int(x+w)]\n        \n        # -- Keypoints: convert to local coords within the crop\n        keypoints_local = self.extract_local_keypoints(ann['keypoints'], bbox)\n        \n        # -- Apply data augmentation (scale, rotate, flip)\n        cropped_aug, keypoints_aug = self.apply_augmentation_with_keypoints(\n            cropped, keypoints_local\n        )\n\n        # -- Resize to final network input size (256×192)\n        final_img = cv2.resize(cropped_aug, (self.input_size[1], self.input_size[0]))\n        \n        # -- Scale the keypoints from (cropped_aug.shape) to (256×192)\n        H_aug, W_aug = cropped_aug.shape[:2]\n        scale_x = self.input_size[1] / W_aug\n        scale_y = self.input_size[0] / H_aug\n        \n        # Now we generate 1/4-scale heatmaps => 64×48\n        keypoints_scaled = []\n        for (kx, ky, v) in keypoints_aug:\n            # First scale up to (256,192)\n            new_x = kx * scale_x\n            new_y = ky * scale_y\n            # Then scale down to (64,48)\n            final_x = new_x * (48 / 192)\n            final_y = new_y * (64 / 256)\n            keypoints_scaled.append((final_x, final_y, v))\n\n        heatmaps = self.generate_heatmaps(keypoints_scaled, (64,48))\n\n        # Convert to torch tensor\n        final_img = torch.from_numpy(final_img).permute(2,0,1).float()\n        heatmaps = torch.from_numpy(heatmaps).float()\n\n        if self.transform:\n            final_img = self.transform(final_img)\n\n        return final_img, heatmaps\n\n    # ----------------------------------------------------------------\n    # HELPER METHODS\n    # ----------------------------------------------------------------\n    def _load_and_filter_coco(self, annotations_json):\n        \"\"\"\n        1. Parse the COCO JSON\n        2. Build a map of image_id -> (file_name, width, height)\n        3. For each annotation (person, iscrowd=0, 17 keypoints),\n           adjust bounding box to 4:3, clip to image edges,\n           and check if it's >= min_box_side in width/height.\n           If valid, store annotation in self.annotations.\n        \"\"\"\n        with open(annotations_json, 'r') as f:\n            coco_data = json.load(f)\n        \n        # image_id -> (file_name, width, height)\n        image_dict = {}\n        for img_info in coco_data[\"images\"]:\n            image_dict[img_info[\"id\"]] = {\n                \"file_name\": img_info[\"file_name\"],\n                \"width\":     img_info[\"width\"],\n                \"height\":    img_info[\"height\"]\n            }\n\n        filtered_anns = []\n        for ann in coco_data[\"annotations\"]:\n            # Check basic conditions\n            if (ann[\"category_id\"] != 1 or\n                ann[\"iscrowd\"] != 0 or\n                \"keypoints\" not in ann or\n                len(ann[\"keypoints\"]) != 51):\n                continue\n\n            num_visible = sum(v > 0 for v in ann[\"keypoints\"][2::3])\n            if num_visible < 5:\n                continue\n\n\n            img_id = ann[\"image_id\"]\n            img_info = image_dict[img_id]\n            orig_bbox = ann[\"bbox\"]  # [x,y,w,h] in COCO coords\n            \n            # Adjust to 4:3 ratio, clip to boundaries\n            adj_bbox = self.adjust_aspect_ratio(orig_bbox, \n                                                img_info[\"width\"], \n                                                img_info[\"height\"])\n            x, y, w, h = adj_bbox\n            \n            # Skip if it is too small\n            if w < self.min_box_side or h < self.min_box_side:\n                continue\n            \n            # Keep it\n            filtered_anns.append({\n                \"file_name\": img_info[\"file_name\"],\n                \"bbox\": [x, y, w, h],\n                \"keypoints\": ann[\"keypoints\"]\n            })\n        \n        print(f\"Filtered annotations: from {len(coco_data['annotations'])} down to {len(filtered_anns)}\")\n        return filtered_anns\n\n    def adjust_aspect_ratio(self, bbox, img_w, img_h, target_ar=4/3):\n        \"\"\"Extend or shrink bbox to 4:3, then clip to image boundaries.\"\"\"\n        x, y, w, h = bbox\n        current_ar = h / w\n        \n        if current_ar < target_ar:\n            # Extend height\n            new_h = w * target_ar\n            center_y = y + h/2\n            y = center_y - new_h/2\n            h = new_h\n        else:\n            # Extend width\n            new_w = h / target_ar\n            center_x = x + w/2\n            x = center_x - new_w/2\n            w = new_w\n        \n        # Clip\n        x = max(0, x)\n        y = max(0, y)\n        w = min(img_w - x, w)\n        h = min(img_h - y, h)\n        \n        return [x, y, w, h]\n    \n    def extract_local_keypoints(self, keypoints_global, bbox):\n        \"\"\"\n        Convert absolute keypoints [x,y,v] to local coords \n        relative to the top-left corner of the bounding box.\n        \"\"\"\n        x0, y0, w, h = bbox\n        keypoints_local = []\n        for i in range(self.num_keypoints):\n            gx = keypoints_global[3*i]\n            gy = keypoints_global[3*i + 1]\n            v  = keypoints_global[3*i + 2]  # visibility\n            local_x = gx - x0\n            local_y = gy - y0\n            keypoints_local.append((local_x, local_y, v))\n        return keypoints_local\n\n    def apply_augmentation_with_keypoints(self, cropped_img, keypoints_local):\n        \"\"\"\n        Apply random scale, rotation, and horizontal flip to \n        both the cropped image and the local keypoints.\n        \"\"\"\n        H, W = cropped_img.shape[:2]\n        \n        # 1) Random scale\n        scale_factor = 1.0 + random.uniform(-self.scale_aug, self.scale_aug)\n        new_W = int(W * scale_factor)\n        new_H = int(H * scale_factor)\n        new_W = max(1, new_W)  # avoid zero dimension\n        new_H = max(1, new_H)\n\n        resized_img = cv2.resize(cropped_img, (new_W, new_H))\n        kp_scaled = []\n        for (kx, ky, v) in keypoints_local:\n            kp_scaled.append((kx * scale_factor, ky * scale_factor, v))\n\n        # 2) Random rotation\n        angle = random.uniform(-self.rotation_aug, self.rotation_aug)\n        center = (new_W / 2, new_H / 2)\n        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n        rotated_img = cv2.warpAffine(resized_img, M, (new_W, new_H))\n\n        kp_rotated = []\n        for (kx, ky, v) in kp_scaled:\n            px = M[0,0]*kx + M[0,1]*ky + M[0,2]\n            py = M[1,0]*kx + M[1,1]*ky + M[1,2]\n            kp_rotated.append((px, py, v))\n\n        # 3) Random horizontal flip\n        final_img = rotated_img\n        kp_final = kp_rotated\n        if random.random() < self.flip_prob:\n            final_img = cv2.flip(rotated_img, 1)\n            kp_flipped = []\n            for (kx, ky, v) in kp_rotated:\n                flipped_x = (new_W - 1) - kx\n                kp_flipped.append((flipped_x, ky, v))\n            kp_final = kp_flipped\n        \n        return final_img, kp_final\n\n    def generate_heatmaps(self, keypoints, heatmap_size):\n        \"\"\"\n        Create 'num_keypoints' heatmaps of shape (heatmap_size[0], heatmap_size[1]).\n        e.g., (64,48).\n        \"\"\"\n        H, W = heatmap_size\n        heatmaps = np.zeros((self.num_keypoints, H, W), dtype=np.float32)\n\n        for k_id in range(self.num_keypoints):\n            kx, ky, v = keypoints[k_id]\n            if v > 0:  # visible or labeled\n                cx = int(round(kx))\n                cy = int(round(ky))\n                self._draw_gaussian(heatmaps[k_id], (cx, cy), sigma=self.sigma)\n        return heatmaps\n\n    def _draw_gaussian(self, heatmap, center, sigma=2):\n        x, y = center\n        H, W = heatmap.shape\n        size = int(3 * sigma)\n        x1, x2 = x - size, x + size + 1\n        y1, y2 = y - size, y + size + 1\n        \n        xx, yy = np.meshgrid(np.arange(x1, x2), np.arange(y1, y2))\n        gaussian = np.exp(-((xx - x)**2 + (yy - y)**2)/(2*sigma**2))\n\n        x1c, x2c = max(0, x1), min(W, x2)\n        y1c, y2c = max(0, y1), min(H, y2)\n\n        if x2c <= x1c or y2c <= y1c:\n            return\n        \n        heatmap_section = heatmap[y1c:y2c, x1c:x2c]\n        gaussian_section = gaussian[(y1c - y1):(y2c - y1), (x1c - x1):(x2c - x1)]\n        np.maximum(heatmap_section, gaussian_section, out=heatmap_section)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T03:35:46.874059Z","iopub.execute_input":"2025-02-06T03:35:46.874381Z","iopub.status.idle":"2025-02-06T03:35:46.898882Z","shell.execute_reply.started":"2025-02-06T03:35:46.874349Z","shell.execute_reply":"2025-02-06T03:35:46.897978Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Class check","metadata":{}},{"cell_type":"code","source":"# # 4.1: Create the dataset and dataloader\n\n# train_dataset = COCOPoseDataset(\n#     annotations_json=COCO_ANNOTATIONS, \n#     images_root=COCO_IMAGES_ROOT,\n#     input_size=(IMG_HEIGHT, IMG_WIDTH), \n#     scale_aug=SCALE_AUG, \n#     rotation_aug=ROTATION_AUG, \n#     flip_prob=FLIP_PROB,\n#     transform=None\n# )\n\n# train_loader = DataLoader(train_dataset, \n#                           batch_size=4, \n#                           shuffle=True, \n#                           )\n\n# # 4.2: Visualize a few samples\n# def visualize_sample(img_tensor, heatmaps_1_4):\n#     \"\"\"\n#     img_tensor: shape (3, 256, 192)\n#     heatmaps_1_4: shape (17, 64, 48)\n#     \"\"\"\n\n#     # Convert the input image to NumPy for plotting\n#     img_np = img_tensor.permute(1,2,0).numpy().astype(np.uint8)\n\n#     # Summation of all keypoint heatmaps (shape: 64×48)\n#     heatmap_sum_small = heatmaps_1_4.numpy().sum(axis=0)  # shape (64, 48)\n\n#     # Upsample the heatmap_sum to 256×192\n#     heatmap_sum_large = cv2.resize(\n#         heatmap_sum_small, \n#         (img_np.shape[1], img_np.shape[0]),  # (width=192, height=256)\n#         interpolation=cv2.INTER_LINEAR\n#     )\n    \n#     fig, ax = plt.subplots(1, 2, figsize=(10,5))\n#     ax[0].imshow(img_np)\n#     ax[0].set_title(\"Augmented Image (256×192)\")\n\n#     ax[1].imshow(img_np, alpha=0.6)\n#     ax[1].imshow(heatmap_sum_large, cmap='jet', alpha=0.5)\n#     ax[1].set_title(\"Overlay: Summed Heatmap Upsampled\")\n#     plt.show()\n\n\n# sample_iter = iter(train_loader)\n# images, hmaps = next(sample_iter)  # shape: (B, 3, 256, 192), (B, 17, 256, 192)\n\n# for i in range(images.size(0)):\n#     visualize_sample(images[i], hmaps[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T03:35:46.899985Z","iopub.execute_input":"2025-02-06T03:35:46.900291Z","iopub.status.idle":"2025-02-06T03:35:46.916009Z","shell.execute_reply.started":"2025-02-06T03:35:46.900269Z","shell.execute_reply":"2025-02-06T03:35:46.915192Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# 3 - Model","metadata":{}},{"cell_type":"code","source":"class SimpleBaselinePose(nn.Module):\n    def __init__(self, backbone='resnet50', num_keypoints=17):\n        super().__init__()\n        # 5.1: Load a ResNet-50 (optionally from torchvision)\n        resnet = resnet50(pretrained=True)\n        \n        # Remove the classification head (fc layer)\n        modules = list(resnet.children())[:-2]  # keep up to layer4's output\n        self.backbone = nn.Sequential(*modules)\n        \n        # 5.2: Add deconvolution layers\n        # The paper typically uses 3 deconv layers, each with 256 filters, kernel=4, stride=2\n        # Output feature map should match (H/4, W/4) upsampled by 2^3=8, leading to HxW final\n        # for input resolution 256x192, the final feature map is also 64x48 in the backbone\n        # so 3 deconv layers of stride 2 will get us back to 256x192\n        self.deconv_layers = self._make_deconv_layer(3, 256)\n        \n        # 5.3: Final conv layer to predict heatmaps\n        self.final_layer = nn.Conv2d(\n            in_channels=256,\n            out_channels=num_keypoints,\n            kernel_size=1,\n            stride=1,\n            padding=0\n        )\n\n    def _make_deconv_layer(self, num_layers, num_filters):\n        layers = []\n        for i in range(num_layers):\n            # Deconv\n            layers.append(\n                nn.ConvTranspose2d(\n                    in_channels = 2048 if i==0 else num_filters, \n                    out_channels= num_filters,\n                    kernel_size=4,\n                    stride=2,\n                    padding=1,\n                    output_padding=0,\n                    bias=False\n                )\n            )\n            layers.append(nn.BatchNorm2d(num_filters))\n            layers.append(nn.ReLU(inplace=True))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.backbone(x)       # shape: (batch, 2048, H/32, W/32) for ResNet50\n        x = self.deconv_layers(x)  # upsample to (batch, 256, H/4, W/4)\n        x = self.final_layer(x)    # shape: (batch, 17, H/4, W/4) or possibly H, W if using 3 upsampling layers\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T03:35:46.918088Z","iopub.execute_input":"2025-02-06T03:35:46.918361Z","iopub.status.idle":"2025-02-06T03:35:46.931877Z","shell.execute_reply.started":"2025-02-06T03:35:46.918334Z","shell.execute_reply":"2025-02-06T03:35:46.931211Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# 4 - Loss","metadata":{}},{"cell_type":"code","source":"# criterion = nn.MSELoss().to(DEVICE)\ncriterion = nn.MSELoss().cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T03:37:48.611195Z","iopub.execute_input":"2025-02-06T03:37:48.61159Z","iopub.status.idle":"2025-02-06T03:37:48.615892Z","shell.execute_reply.started":"2025-02-06T03:37:48.61156Z","shell.execute_reply":"2025-02-06T03:37:48.614887Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# 5 - Model, Optimizer and LR","metadata":{}},{"cell_type":"code","source":"# model = SimpleBaselinePose(backbone='resnet50', num_keypoints=NUM_KEYPOINTS).to(DEVICE)\nmodel = SimpleBaselinePose(backbone='resnet50', num_keypoints=NUM_KEYPOINTS)\n\nmodel = model.cuda()\n\n# If multiple GPUs are detected, wrap with DataParallel\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = nn.DataParallel(model)\n\noptimizer = optim.Adam(model.parameters(), lr=BASE_LR)\n\n# MultiStepLR will drop LR by 10 at each milestone\nlr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=LR_MILESTONES, gamma=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T03:38:49.424231Z","iopub.execute_input":"2025-02-06T03:38:49.424549Z","iopub.status.idle":"2025-02-06T03:38:50.350907Z","shell.execute_reply.started":"2025-02-06T03:38:49.424525Z","shell.execute_reply":"2025-02-06T03:38:50.350147Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# 6 - Training loop","metadata":{}},{"cell_type":"code","source":"train_dataset = COCOPoseDataset(\n    annotations_json=COCO_ANNOTATIONS, \n    images_root=COCO_IMAGES_ROOT,\n    input_size=(IMG_HEIGHT, IMG_WIDTH), \n    scale_aug=SCALE_AUG, \n    rotation_aug=ROTATION_AUG, \n    flip_prob=FLIP_PROB,\n    transform=None\n)\n\nscaler = GradScaler()\n\n# 8.1: Create DataLoader for training, validation\n# Let's say we want to split 90% for training, 10% for validation\ntrain_size = int(0.9 * len(train_dataset))\nval_size = len(train_dataset) - train_size\n\n# random_split will produce two Subset objects\ntrain_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n\nprint(f\"Train subset size: {len(train_subset)}\")\nprint(f\"Val subset size:   {len(val_subset)}\")\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=8)\nval_loader   = DataLoader(val_subset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=8)\n\n# For quick local test only\n# train_loader = DataLoader(train_subset, batch_size=8, shuffle=True)\n# val_loader   = DataLoader(val_subset,   batch_size=8, shuffle=False)\n\n# 8.2: Training Loop\nbest_val_loss = float('inf')\n\nnum_epochs = 140\n# NUM_EPOCHS\nfor epoch in range(2):\n    # ---------------------------\n    # Training\n    # ---------------------------\n    model.train()\n    start_time = time.time()\n    running_loss = 0.0\n    \n    for images, heatmaps in train_loader:\n        # images = images.to(DEVICE)\n        # heatmaps = heatmaps.to(DEVICE)\n        images = images.cuda()     # put the entire batch on GPU:0\n        heatmaps = heatmaps.cuda() # same\n        \n        optimizer.zero_grad()\n\n        with torch.autocast(device_type='cuda', dtype=torch.float16):\n            outputs = model(images)\n            loss = criterion(outputs, heatmaps)        \n        \n        # Scale the loss before backprop\n        scaler.scale(loss).backward()\n        # Update parameters\n        scaler.step(optimizer)\n        # Adjust the scaling factor\n        scaler.update()\n        \n        running_loss += loss.item() * images.size(0)\n    \n    epoch_train_loss = running_loss / len(train_loader.dataset)\n    \n    # ---------------------------\n    # Validation\n    # ---------------------------\n    model.eval()\n    val_running_loss = 0.0\n    \n    with torch.no_grad():\n        for images, heatmaps in val_loader:\n            # images = images.to(DEVICE)\n            # heatmaps = heatmaps.to(DEVICE)\n            images = images.cuda()     # put the entire batch on GPU:0\n            heatmaps = heatmaps.cuda() # same\n\n            outputs = model(images)\n            val_loss = criterion(outputs, heatmaps)\n            \n            val_running_loss += val_loss.item() * images.size(0)\n    \n    epoch_val_loss = val_running_loss / len(val_loader.dataset)\n    \n    # Update LR\n    lr_scheduler.step()\n\n    # Elapsed time for this epoch\n    elapsed_time = time.time() - start_time\n    \n    # Track best model\n    if epoch_val_loss < best_val_loss:\n        best_val_loss = epoch_val_loss\n        torch.save(model.state_dict(), \"best_model.pth\")\n    \n    # Print stats\n    print(f\"Epoch {epoch+1}/{num_epochs} \"\n          f\"Train Loss: {epoch_train_loss:.6f} \"\n          f\"Val Loss: {epoch_val_loss:.6f} \"\n          f\"Time: {elapsed_time:.2f} sec\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T03:38:56.675741Z","iopub.execute_input":"2025-02-06T03:38:56.676058Z","execution_failed":"2025-02-06T04:07:37.703Z"}},"outputs":[{"name":"stdout","text":"Filtered annotations: from 262465 down to 134214\nTrain subset size: 120792\nVal subset size:   13422\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-14-e9eaa18a9ba5>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"}],"execution_count":null}]}